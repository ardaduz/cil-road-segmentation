{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial code taken from https://github.com/tensorflow/models/blob/master/samples/outreach/blogs/segmentation_blogpost/image_segmentation.ipynb\n",
    "with minor modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ODNLPGHKKgr-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQ9VRReUQxXi"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib as tfcontrib\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import backend as K  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wT1kb3q0ghhi"
   },
   "outputs": [],
   "source": [
    "img_dir = '../competition-data/training/images'\n",
    "label_dir = '../competition-data/training/groundtruth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DtutNudKbf70"
   },
   "outputs": [],
   "source": [
    "train_filenames = os.listdir(img_dir)\n",
    "x_train_filenames = [os.path.join(img_dir, filename) for filename in train_filenames]\n",
    "y_train_filenames = [os.path.join(label_dir, filename) for filename in train_filenames]\n",
    "\n",
    "x_train_filenames, x_val_filenames, y_train_filenames, y_val_filenames = train_test_split(x_train_filenames, y_train_filenames, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDycQekHaMqq"
   },
   "outputs": [],
   "source": [
    "num_train_examples = len(x_train_filenames)\n",
    "num_val_examples = len(x_val_filenames)\n",
    "\n",
    "print(\"Number of training examples: {}\".format(num_train_examples))\n",
    "print(\"Number of validation examples: {}\".format(num_val_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nhda5fkPS3JD"
   },
   "source": [
    "### Here's what the paths look like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Di1N83ArilzR"
   },
   "outputs": [],
   "source": [
    "x_train_filenames[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gc-BDv1Zio1z"
   },
   "outputs": [],
   "source": [
    "y_train_filenames[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mhvDoZkbcUa1"
   },
   "source": [
    "# Visualize\n",
    "Let's take a look at some of the examples of different images in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qUA6SDLhozjj"
   },
   "outputs": [],
   "source": [
    "display_num = 5\n",
    "\n",
    "r_choices = np.random.choice(num_train_examples, display_num)\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "for i in range(0, display_num * 2, 2):\n",
    "  img_num = r_choices[i // 2]\n",
    "  x_pathname = x_train_filenames[img_num]\n",
    "  y_pathname = y_train_filenames[img_num]\n",
    "  \n",
    "  plt.subplot(display_num, 2, i + 1)\n",
    "  plt.imshow(mpimg.imread(x_pathname), cmap=\"gray\")  \n",
    "  plt.subplot(display_num, 2, i + 2)\n",
    "  plt.imshow(mpimg.imread(y_pathname), cmap=\"gray\")\n",
    "  \n",
    "plt.suptitle(\"Examples of Images and their Segmentations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d4CPgvPiToB_"
   },
   "source": [
    "# Set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeDoiSFlothe"
   },
   "outputs": [],
   "source": [
    "img_shape = (256, 256, 3)\n",
    "batch_size = 3\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3aGi28u8Cq9M"
   },
   "source": [
    "## Processing each pathname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fb_psznAggwr"
   },
   "outputs": [],
   "source": [
    "def _process_pathnames(fname, label_path):\n",
    "  # We map this function onto each pathname pair  \n",
    "  img_str = tf.read_file(fname)\n",
    "  img = tf.image.decode_png(img_str, channels=3)\n",
    "\n",
    "  label_img_str = tf.read_file(label_path)\n",
    "  label_img = tf.image.decode_png(label_img_str, channels=1)\n",
    "  return img, label_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4UE28JiCuOk"
   },
   "source": [
    "## Shifting the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdY046OqtGVH"
   },
   "outputs": [],
   "source": [
    "def shift_img(output_img, label_img, width_shift_range, height_shift_range):\n",
    "  \"\"\"This fn will perform the horizontal or vertical shift\"\"\"\n",
    "  if width_shift_range or height_shift_range:\n",
    "      if width_shift_range:\n",
    "        width_shift_range = tf.random_uniform([], \n",
    "                                              -width_shift_range * img_shape[1],\n",
    "                                              width_shift_range * img_shape[1])\n",
    "      if height_shift_range:\n",
    "        height_shift_range = tf.random_uniform([],\n",
    "                                               -height_shift_range * img_shape[0],\n",
    "                                               height_shift_range * img_shape[0])\n",
    "      # Translate both \n",
    "      output_img = tfcontrib.image.translate(output_img,\n",
    "                                             [width_shift_range, height_shift_range])\n",
    "      label_img = tfcontrib.image.translate(label_img,\n",
    "                                             [width_shift_range, height_shift_range])\n",
    "  return output_img, label_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qY253aZfCwd2"
   },
   "source": [
    "## Flipping the image randomly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OogLSplstur9"
   },
   "outputs": [],
   "source": [
    "def flip_img(horizontal_flip, tr_img, label_img):\n",
    "  if horizontal_flip:\n",
    "    flip_prob = tf.random_uniform([], 0.0, 1.0)\n",
    "    tr_img, label_img = tf.cond(tf.less(flip_prob, 0.5),\n",
    "                                lambda: (tf.image.flip_left_right(tr_img), tf.image.flip_left_right(label_img)),\n",
    "                                lambda: (tr_img, label_img))\n",
    "  return tr_img, label_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qY253aZfCwd2"
   },
   "source": [
    "## Rotating the image randomly (90° angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OogLSplstur9"
   },
   "outputs": [],
   "source": [
    "def rot90_img(rotate90, tr_img, label_img):\n",
    "  if rotate90:\n",
    "    rotate_count = tf.random_uniform([], 0, 4, dtype=tf.dtypes.int32)\n",
    "    tr_img = tf.image.rot90(tr_img, rotate_count)\n",
    "    label_img = tf.image.rot90(label_img, rotate_count)\n",
    "    \n",
    "  return tr_img, label_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_YIJLIr5Cyyr"
   },
   "source": [
    "## Assembling our transformations into our augment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "18WA0Sl3olyn"
   },
   "outputs": [],
   "source": [
    "def _augment(img,\n",
    "             label_img,\n",
    "             resize=None,  # Resize the image to some size e.g. [256, 256]\n",
    "             scale=1,  # Scale image e.g. 1 / 255.\n",
    "             hue_delta=0,  # Adjust the hue of an RGB image by random factor\n",
    "             horizontal_flip=False,  # Random left right flip,\n",
    "             rotate90=False,  # Random number of 90° rotations\n",
    "             width_shift_range=0,  # Randomly translate the image horizontally\n",
    "             height_shift_range=0):  # Randomly translate the image vertically \n",
    "  if resize is not None:\n",
    "    # Resize both images\n",
    "    label_img = tf.image.resize_images(label_img, resize)\n",
    "    img = tf.image.resize_images(img, resize)\n",
    "  \n",
    "  if hue_delta:\n",
    "    img = tf.image.random_hue(img, hue_delta)\n",
    "  \n",
    "  img, label_img = flip_img(horizontal_flip, img, label_img)\n",
    "  img, label_img = rot90_img(rotate90, img, label_img)\n",
    "  img, label_img = shift_img(img, label_img, width_shift_range, height_shift_range)\n",
    "  label_img = tf.cast(label_img, tf.dtypes.float32) * scale\n",
    "  img = tf.cast(img, tf.dtypes.float32) * scale \n",
    "  return img, label_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tkNqQaR2HQbd"
   },
   "outputs": [],
   "source": [
    "def get_baseline_dataset(filenames, \n",
    "                         labels,\n",
    "                         preproc_fn=functools.partial(_augment),\n",
    "                         threads=5, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True):           \n",
    "  num_x = len(filenames)\n",
    "  # Create a dataset from the filenames and labels\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "  # Map our preprocessing function to every element in our dataset, taking\n",
    "  # advantage of multithreading\n",
    "  dataset = dataset.map(_process_pathnames, num_parallel_calls=threads)\n",
    "  if preproc_fn.keywords is not None and 'resize' not in preproc_fn.keywords:\n",
    "    assert batch_size == 1, \"Batching images must be of the same size\"\n",
    "\n",
    "  dataset = dataset.map(preproc_fn, num_parallel_calls=threads)\n",
    "  \n",
    "  if shuffle:\n",
    "    dataset = dataset.shuffle(num_x)\n",
    "  \n",
    "  \n",
    "  # It's necessary to repeat our data for all epochs \n",
    "  dataset = dataset.repeat().batch(batch_size)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zwtgius5CRKc"
   },
   "source": [
    "## Set up train and validation datasets\n",
    "Note that we apply image augmentation to our training dataset but not our validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iu5WmYmOwKrV"
   },
   "outputs": [],
   "source": [
    "tr_cfg = {\n",
    "    'resize': [img_shape[0], img_shape[1]],\n",
    "    'scale': 1 / 255.,\n",
    "    'horizontal_flip': True,\n",
    "    'rotate90': True,\n",
    "}\n",
    "tr_preprocessing_fn = functools.partial(_augment, **tr_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtzLkDFMpF0T"
   },
   "outputs": [],
   "source": [
    "val_cfg = {\n",
    "    'resize': [img_shape[0], img_shape[1]],\n",
    "    'scale': 1 / 255.,\n",
    "}\n",
    "val_preprocessing_fn = functools.partial(_augment, **val_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5cNpECdkaafo"
   },
   "outputs": [],
   "source": [
    "train_ds = get_baseline_dataset(x_train_filenames,\n",
    "                                y_train_filenames,\n",
    "                                preproc_fn=tr_preprocessing_fn,\n",
    "                                batch_size=batch_size)\n",
    "val_ds = get_baseline_dataset(x_val_filenames,\n",
    "                              y_val_filenames, \n",
    "                              preproc_fn=val_preprocessing_fn,\n",
    "                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yasuvr5IbFlM"
   },
   "source": [
    "## Let's see if our image augmentor data pipeline is producing expected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hjoUqbPdHQej"
   },
   "outputs": [],
   "source": [
    "temp_ds = get_baseline_dataset(x_train_filenames, \n",
    "                               y_train_filenames,\n",
    "                               preproc_fn=tr_preprocessing_fn,\n",
    "                               batch_size=1,\n",
    "                               shuffle=False)\n",
    "# Let's examine some of these augmented images\n",
    "data_aug_iter = temp_ds.make_one_shot_iterator()\n",
    "next_element = data_aug_iter.get_next()\n",
    "with tf.Session() as sess: \n",
    "  batch_of_imgs, label = sess.run(next_element)\n",
    "\n",
    "  # Running next element in our graph will produce a batch of images\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  img = batch_of_imgs[0]\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.imshow(img)\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.imshow(label[0, :, :, 0], cmap='gray')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zfew1i1F6bK-"
   },
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, num_filters):\n",
    "  encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
    "  encoder = layers.BatchNormalization()(encoder)\n",
    "  encoder = layers.Activation('relu')(encoder)\n",
    "  encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
    "  encoder = layers.BatchNormalization()(encoder)\n",
    "  encoder = layers.Activation('relu')(encoder)\n",
    "  return encoder\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "  encoder = conv_block(input_tensor, num_filters)\n",
    "  encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
    "  \n",
    "  return encoder_pool, encoder\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
    "  decoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
    "  decoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
    "  decoder = layers.BatchNormalization()(decoder)\n",
    "  decoder = layers.Activation('relu')(decoder)\n",
    "  decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "  decoder = layers.BatchNormalization()(decoder)\n",
    "  decoder = layers.Activation('relu')(decoder)\n",
    "  decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "  decoder = layers.BatchNormalization()(decoder)\n",
    "  decoder = layers.Activation('relu')(decoder)\n",
    "  return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRLp21S_hpTn"
   },
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=img_shape)\n",
    "# 256\n",
    "\n",
    "encoder0_pool, encoder0 = encoder_block(inputs, 32)\n",
    "# 128\n",
    "\n",
    "encoder1_pool, encoder1 = encoder_block(encoder0_pool, 64)\n",
    "# 64\n",
    "\n",
    "encoder2_pool, encoder2 = encoder_block(encoder1_pool, 128)\n",
    "# 32\n",
    "\n",
    "encoder3_pool, encoder3 = encoder_block(encoder2_pool, 256)\n",
    "# 16\n",
    "\n",
    "encoder4_pool, encoder4 = encoder_block(encoder3_pool, 512)\n",
    "# 8\n",
    "\n",
    "center = conv_block(encoder4_pool, 1024)\n",
    "# center\n",
    "\n",
    "decoder4 = decoder_block(center, encoder4, 512)\n",
    "# 16\n",
    "\n",
    "decoder3 = decoder_block(decoder4, encoder3, 256)\n",
    "# 32\n",
    "\n",
    "decoder2 = decoder_block(decoder3, encoder2, 128)\n",
    "# 64\n",
    "\n",
    "decoder1 = decoder_block(decoder2, encoder1, 64)\n",
    "# 128\n",
    "\n",
    "decoder0 = decoder_block(decoder1, encoder0, 32)\n",
    "# 256\n",
    "\n",
    "outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "luDqDqu8c1AX"
   },
   "source": [
    "## Define your model\n",
    "Using functional API, you must define your model by specifying the inputs and outputs associated with the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76QkTzXVczgc"
   },
   "outputs": [],
   "source": [
    "model = models.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p0tNnmyOdtyr"
   },
   "source": [
    "# Defining custom metrics and loss functions\n",
    "Defining loss and metric functions are simple with Keras. Simply define a function that takes both the True labels for a given example and the Predicted labels for the same given example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfuBVut0fogM"
   },
   "source": [
    "Dice loss is a metric that measures overlap. More info on optimizing for Dice coefficient (our dice loss) can be found in the [paper](http://campar.in.tum.de/pub/milletari2016Vnet/milletari2016Vnet.pdf), where it was introduced. \n",
    "\n",
    "We use dice loss here because it performs better at class imbalanced problems by design. In addition, maximizing the dice coefficient and IoU metrics are the actual objectives and goals of our segmentation task. Using cross entropy is more of a proxy which is easier to maximize. Instead, we maximize our objective directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_8_hbHECUAW"
   },
   "outputs": [],
   "source": [
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    # Flatten\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DgINhlpNaxP"
   },
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqClGNFJdANU"
   },
   "source": [
    "Here, we'll use a specialized loss function that combines binary cross entropy and our dice loss. This is based on [individuals who competed within this competition obtaining better results empirically](https://www.kaggle.com/c/carvana-image-masking-challenge/discussion/40199). Try out your own custom losses to measure performance (e.g. bce + log(dice_loss), only bce, etc.)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "udrfi9JGB-bL"
   },
   "outputs": [],
   "source": [
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    loss = tf.sqrt(losses.mean_squared_error(y_true, y_pred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LifmpjXNc9Gz"
   },
   "source": [
    "## Compile your model\n",
    "We use our custom loss function to minimize. In addition, we specify what metrics we want to keep track of as we train. Note that metrics are not actually used during the training process to tune the parameters, but are instead used to measure performance of the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gflcWk2Cc8Bi",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_loss, root_mean_squared_error])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8WG_8iZ_dMbK"
   },
   "source": [
    "## Train your model\n",
    "Training your model with `tf.data` involves simply providing the model's `fit` function with your training/validation dataset, the number of steps, and epochs.  \n",
    "\n",
    "We also include a Model callback, [`ModelCheckpoint`](https://keras.io/callbacks/#modelcheckpoint) that will save the model to disk after each epoch. We configure it such that it only saves our highest performing model. Note that saving the model capture more than just the weights of the model: by default, it saves the model architecture, weights, as well as information about the training process such as the state of the optimizer, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1nHnj6199elZ"
   },
   "outputs": [],
   "source": [
    "save_model_path = 'weights.hdf5'\n",
    "cp = tf.keras.callbacks.ModelCheckpoint(filepath=save_model_path, monitor='val_root_mean_squared_error', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJP_EvuTb4hH"
   },
   "source": [
    "Don't forget to specify our model callback in the `fit` function call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMZcOrq5aaj1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, \n",
    "                   steps_per_epoch=int(np.ceil(num_train_examples / float(batch_size))),\n",
    "                   epochs=epochs,\n",
    "                   validation_data=val_ds,\n",
    "                   validation_steps=int(np.ceil(num_val_examples / float(batch_size))),\n",
    "                   callbacks=[cp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCAUsoxfTTrh"
   },
   "source": [
    "# Visualize training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvntxymYn8rM"
   },
   "outputs": [],
   "source": [
    "rmse = history.history['root_mean_squared_error']\n",
    "val_rmse = history.history['val_root_mean_squared_error']\n",
    "\n",
    "dice = history.history['dice_loss']\n",
    "val_dice = history.history['val_dice_loss']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, dice, label='Training Dice Loss')\n",
    "plt.plot(epochs_range, val_dice, label='Validation Dice Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Dice Loss')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs_range, rmse, label='Training RMSE')\n",
    "plt.plot(epochs_range, val_rmse, label='Validation RMSE')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation RMSE Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dWPhb87GdhkG"
   },
   "source": [
    "Even with only 5 epochs, we see strong performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGFKf8yCTYbw"
   },
   "source": [
    "# Visualize actual performance \n",
    "We'll visualize our performance on the validation set.\n",
    "\n",
    "Note that in an actual setting (competition, deployment, etc.) we'd evaluate on the test set with the full image resolution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIddsUcM_KeI"
   },
   "source": [
    "To load our model we have two options:\n",
    "1. Since our model architecture is already in memory, we can simply call `load_weights(save_model_path)`\n",
    "2. If you wanted to load the model from scratch (in a different setting without already having the model architecture in memory) we simply call \n",
    "\n",
    "```model = models.load_model(save_model_path, custom_objects={'bce_dice_loss': bce_dice_loss, 'dice_loss': dice_loss})```, specificing the necessary custom objects, loss and metrics, that we used to train our model. \n",
    "\n",
    "If you want to see more examples, check our the [keras guide](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Ph7acmrCXm6"
   },
   "outputs": [],
   "source": [
    "# Alternatively, load the weights directly: model.load_weights(save_model_path)\n",
    "model = models.load_model(save_model_path, custom_objects={'root_mean_squared_error': root_mean_squared_error,\n",
    "                                                           'bce_dice_loss': bce_dice_loss,\n",
    "                                                           'dice_loss': dice_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0GnwZ7CPaamI"
   },
   "outputs": [],
   "source": [
    "# Let's visualize some of the outputs \n",
    "data_aug_iter = val_ds.make_one_shot_iterator()\n",
    "next_element = data_aug_iter.get_next()\n",
    "\n",
    "# Running next element in our graph will produce a batch of images\n",
    "plt.figure(figsize=(10, 20))\n",
    "for i in range(5):\n",
    "  batch_of_imgs, label = tf.keras.backend.get_session().run(next_element)\n",
    "  img = batch_of_imgs[0]\n",
    "  predicted_label = model.predict(batch_of_imgs)[0]\n",
    "\n",
    "  plt.subplot(5, 3, 3 * i + 1)\n",
    "  plt.imshow(img)\n",
    "  plt.title(\"Input image\")\n",
    "  \n",
    "  plt.subplot(5, 3, 3 * i + 2)\n",
    "  plt.imshow(label[0, :, :, 0], cmap='gray')\n",
    "  plt.title(\"Actual Mask\")\n",
    "  plt.subplot(5, 3, 3 * i + 3)\n",
    "  plt.imshow(predicted_label[:, :, 0], cmap='gray')\n",
    "  plt.title(\"Predicted Mask\")\n",
    "plt.suptitle(\"Examples of Input Image, Label, and Prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Image Segmentation",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
