{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ODNLPGHKKgr-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import functools\n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "#mpl.rcParams['axes.grid'] = False\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.colors import Normalize\n",
    "import pandas as pd\n",
    "import imageio\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQ9VRReUQxXi"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib as tfcontrib\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom TensorBoard callback taken from https://stackoverflow.com/a/48393723\n",
    "\n",
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        \n",
    "        # Pass the non-validation logs to `TensorBoard.on_epoch_end`\n",
    "        train_logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, train_logs)\n",
    "        \n",
    "        val_logs = {k.replace('val_', 'epoch_'): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d4CPgvPiToB_"
   },
   "source": [
    "# Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeDoiSFlothe"
   },
   "outputs": [],
   "source": [
    "img_shape = [256, 256, 3]\n",
    "batch_size = 4\n",
    "epochs = 200\n",
    "cv_splits = 1\n",
    "\n",
    "DEBUG_OUTPUT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fb_psznAggwr"
   },
   "outputs": [],
   "source": [
    "def _process_pathnames(fname, label_path):\n",
    "  \"\"\"Load the image pair from given paths\"\"\"\n",
    "  # We map this function onto each pathname pair  \n",
    "  img_str = tf.read_file(fname)\n",
    "  img = tf.image.decode_png(img_str, channels=3)\n",
    "\n",
    "  label_img_str = tf.read_file(label_path)\n",
    "  label_img = tf.image.decode_png(label_img_str, channels=1)\n",
    "  \n",
    "  return img, label_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4UE28JiCuOk"
   },
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "18WA0Sl3olyn"
   },
   "outputs": [],
   "source": [
    "def _augment_extract_training_patches(img, label_img, extract_patch_size, rotate):\n",
    "  batch_size = tf.shape(img)[0]\n",
    "  img_height = tf.shape(img)[1]\n",
    "  img_width = tf.shape(img)[2]\n",
    "    \n",
    "  if extract_patch_size is not None:    \n",
    "    patch_height = extract_patch_size[0]\n",
    "    patch_width = extract_patch_size[1]\n",
    "    patch_size = tf.cast([patch_height, patch_width], dtype=tf.int32)\n",
    "        \n",
    "    if rotate is not None:\n",
    "      # Extract larger patches first, then rotate, then extract centered patch of correct size.\n",
    "      rotate = float(rotate)\n",
    "      angle = tf.random.uniform((), minval=-rotate, maxval=rotate)\n",
    "      patch_height = tf.ceil(tf.abs(extract_patch_size[1] * tf.sin(angle)) + tf.abs(extract_patch_size[0] * tf.cos(angle)))\n",
    "      patch_width = tf.ceil(tf.abs(extract_patch_size[1] * tf.cos(angle)) + tf.abs(extract_patch_size[0] * tf.sin(angle)))\n",
    "      patch_size = tf.cast([patch_height, patch_width], dtype=tf.int32)\n",
    "    \n",
    "    half_patch_height = patch_height / 2\n",
    "    offset_x = tf.random.uniform([batch_size], \n",
    "                                 minval=half_patch_height, \n",
    "                                 maxval=tf.cast(img_height, dtype=tf.float32) - half_patch_height)\n",
    "    half_patch_width = patch_width / 2\n",
    "    offset_y = tf.random.uniform([batch_size], \n",
    "                                 minval=half_patch_width, \n",
    "                                 maxval=tf.cast(img_width, dtype=tf.float32) - half_patch_width)\n",
    "    offsets = tf.stack([offset_x, offset_y], axis=1)\n",
    "    offsets = tf.cast(offsets, dtype=tf.float32)\n",
    "  \n",
    "    img = tf.image.extract_glimpse(img, patch_size, offsets, centered=False, normalized=False)\n",
    "    label_img = tf.image.extract_glimpse(label_img, patch_size, offsets, centered=False, normalized=False)\n",
    "  \n",
    "  if rotate is not None:\n",
    "    if angle is None:\n",
    "      angle = float(rotate)\n",
    "    img = tf.contrib.image.rotate(img, angle)\n",
    "    label_img = tf.contrib.image.rotate(label_img, angle)\n",
    "    \n",
    "    if extract_patch_size is not None:\n",
    "      # crop centered patches\n",
    "      offsets = [half_patch_height, half_patch_width]\n",
    "      offsets = tf.tile(tf.expand_dims(offsets, axis=0), [batch_size, 1])\n",
    "      offsets = tf.cast(offsets, dtype=tf.float32)\n",
    "      patch_size = tf.cast([extract_patch_size[0], extract_patch_size[1]], dtype=tf.int32)\n",
    "      \n",
    "      img = tf.image.extract_glimpse(img, patch_size, offsets, centered=False, normalized=False)\n",
    "      label_img = tf.image.extract_glimpse(label_img, patch_size, offsets, centered=False, normalized=False)\n",
    "      \n",
    "  return img, label_img\n",
    "  \n",
    "def _augment_extract_validation_patches(img, label_img, extract_patch_size):\n",
    "  patch_size = tf.cast([extract_patch_size[0], extract_patch_size[1]], dtype=tf.int32)\n",
    "  batch_size = tf.shape(img)[0]\n",
    "  img_height = tf.cast(tf.shape(img)[1], dtype=tf.float32)\n",
    "  img_width = tf.cast(tf.shape(img)[2], dtype=tf.float32)\n",
    "    \n",
    "  half_height = tf.cast(extract_patch_size[0] / 2, dtype=tf.float32)\n",
    "  xs = tf.linspace(half_height,\n",
    "                   img_height - half_height, \n",
    "                   tf.cast(tf.ceil(img_width / extract_patch_size[0]), dtype=tf.int32))\n",
    "  \n",
    "  half_width = tf.cast(extract_patch_size[1] / 2, dtype=tf.float32)\n",
    "  ys = tf.linspace(half_width, \n",
    "                   img_width - half_width, \n",
    "                   tf.cast(tf.ceil(img_width / extract_patch_size[1]), dtype=tf.int32))\n",
    "  xx, yy = tf.meshgrid(xs, ys)\n",
    "  xy = tf.stack([tf.reshape(xx, [-1]), tf.reshape(yy, [-1])], axis=1)\n",
    "  xy = tf.cast(xy, dtype=tf.float32)\n",
    "  \n",
    "  num_patches = tf.shape(xy)[0]\n",
    "  # new batch size is batch_size * num_patches\n",
    "  batch_size = batch_size * num_patches\n",
    "  \n",
    "  img = tf.map_fn(lambda i: tf.image.extract_glimpse(\n",
    "    tf.tile(tf.expand_dims(i, axis=0), [num_patches, 1, 1, 1]), \n",
    "    patch_size, xy, centered=False, normalized=False), img)\n",
    "  # img has shape [batch_size, num_patches, height, width, num_channels]\n",
    "  shape = tf.shape(img)\n",
    "  img = tf.reshape(img, [batch_size, shape[2], shape[3], shape[4]])\n",
    "  \n",
    "  label_img = tf.map_fn(lambda i: tf.image.extract_glimpse(\n",
    "    tf.tile(tf.expand_dims(i, axis=0), [num_patches, 1, 1, 1]),\n",
    "    patch_size, xy, centered=False, normalized=False), label_img)\n",
    "  # label_img has shape [batch_size, num_patches, height, width, num_channels]\n",
    "  shape = tf.shape(label_img)\n",
    "  label_img = tf.reshape(label_img, [batch_size, shape[2], shape[3], shape[4]])\n",
    "    \n",
    "  return img, label_img, batch_size\n",
    "\n",
    "def _augment_flip(img, label_img, flip, is_training):\n",
    "  batch_size = tf.shape(img)[0]\n",
    "  \n",
    "  if flip:\n",
    "    if is_training:\n",
    "      chance = tf.random.uniform([batch_size], minval=0.0, maxval=1.0)\n",
    "      img = tf.map_fn(lambda x: tf.cond(x[1] > 0.5, lambda: tf.image.flip_left_right(x[0]), lambda: x[0]), \n",
    "                      (img, chance),\n",
    "                      dtype=tf.float32)\n",
    "      label_img = tf.map_fn(lambda x: tf.cond(x[1] > 0.5, lambda: tf.image.flip_left_right(x[0]), lambda: x[0]), \n",
    "                            (label_img, chance),\n",
    "                            dtype=tf.float32)\n",
    "    else:\n",
    "      tmp = [img, tf.image.flip_left_right(img)]\n",
    "      img = tf.concat(tmp, axis=0)\n",
    "      tmp = [label_img, tf.image.flip_left_right(label_img)]\n",
    "      label_img = tf.concat(tmp, axis=0)\n",
    "\n",
    "  return img, label_img, batch_size\n",
    "\n",
    "def _augment_rot90(img, label_img, rotate90, is_training):\n",
    "  batch_size = tf.shape(img)[0]\n",
    "  \n",
    "  if rotate90:\n",
    "    if is_training:\n",
    "      rotate_count = tf.random_uniform([batch_size], minval=0, maxval=4, dtype=tf.int32)\n",
    "      img = tf.map_fn(lambda x: tf.image.rot90(x[0], x[1]), (img, rotate_count), dtype=tf.float32)\n",
    "      label_img = tf.map_fn(lambda x: tf.image.rot90(x[0], x[1]), (label_img, rotate_count), dtype=tf.float32)\n",
    "      \n",
    "    else:\n",
    "      # Concatenate all four 90° rotations\n",
    "      tmp = [tf.image.rot90(img, i) for i in range(4)]\n",
    "      img = tf.concat(tmp, axis=0)\n",
    "      tmp = [tf.image.rot90(label_img, i) for i in range(4)]\n",
    "      label_img = tf.concat(tmp, axis=0)\n",
    "      batch_size = tf.shape(img)[0]\n",
    "      \n",
    "  return img, label_img, batch_size\n",
    "\n",
    "def _augment(img,\n",
    "             label_img,\n",
    "             extract_patch_size=None,\n",
    "             rotate=None,\n",
    "             resize=None,\n",
    "             scale=1.0,\n",
    "             hue_delta=None,\n",
    "             contrast_factor_upper_bound = None,\n",
    "             saturation_factor_upper_bound = None,\n",
    "             flip=False,\n",
    "             rotate90=False,\n",
    "             is_training=False):\n",
    "  \"\"\"Apply data augmentation on batch\"\"\"\n",
    "  \n",
    "  batch_size = tf.shape(img)[0]\n",
    "  \n",
    "  # Scale\n",
    "  img = tf.cast(img, tf.float32) * scale\n",
    "  label_img = tf.cast(label_img, tf.float32) * scale\n",
    "  \n",
    "  if is_training:\n",
    "    img, label_img = _augment_extract_training_patches(img, label_img, extract_patch_size, rotate)\n",
    "  else:\n",
    "    img, label_img, batch_size = _augment_extract_validation_patches(img, label_img, extract_patch_size)\n",
    "    \n",
    "  # Hue\n",
    "  if hue_delta is not None:\n",
    "    hue_delta = float(hue_delta)\n",
    "    img = tf.image.random_hue(img, max_delta=hue_delta)\n",
    "\n",
    "  # Contrast\n",
    "  if contrast_factor_upper_bound is not None:\n",
    "    upper_bound = float(contrast_factor_upper_bound)\n",
    "    lower_bound = 1 / upper_bound\n",
    "    img = tf.image.random_contrast(img, lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "  # Saturation\n",
    "  if saturation_factor_upper_bound is not None:\n",
    "    upper_bound = float(saturation_factor_upper_bound)\n",
    "    lower_bound = 1 / upper_bound\n",
    "    img = tf.image.random_saturation(img, lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "  # Ensure images are in [0, 1] range\n",
    "  img = tf.minimum(tf.maximum(img, 0.0), 1.0)\n",
    "  # Ensure pixels in label_img are either 0.0 or 1.0\n",
    "  label_img = tf.cast(tf.cast(label_img > 0.5, dtype=tf.int32), dtype=tf.float32)\n",
    "    \n",
    "  # Random flip\n",
    "  img, label_img, batch_size = _augment_flip(img, label_img, flip, is_training)\n",
    "  # Random 90° rotations\n",
    "  img, label_img, batch_size = _augment_rot90(img, label_img, rotate90, is_training)\n",
    "  \n",
    "  return img, label_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tkNqQaR2HQbd"
   },
   "outputs": [],
   "source": [
    "def get_baseline_dataset(filenames, \n",
    "                         labels,\n",
    "                         preproc_fn=functools.partial(_augment),\n",
    "                         threads=6, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True,\n",
    "                         repeat=True):           \n",
    "  num_x = len(filenames)\n",
    "  # Create a dataset from the filenames and labels\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "  dataset = dataset.map(_process_pathnames, num_parallel_calls=threads)\n",
    "  \n",
    "  if shuffle:\n",
    "    dataset = dataset.shuffle(num_x)\n",
    "  \n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.map(preproc_fn, num_parallel_calls=threads)\n",
    "  \n",
    "  # It's necessary to repeat our data for all epochs\n",
    "  if repeat:\n",
    "    dataset = dataset.repeat()    \n",
    "  \n",
    "  dataset = dataset.prefetch(1)\n",
    "  \n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wT1kb3q0ghhi"
   },
   "outputs": [],
   "source": [
    "img_dir = '../competition-data/training/images'\n",
    "label_dir = '../competition-data/training/groundtruth'\n",
    "test_dir = '../competition-data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DtutNudKbf70"
   },
   "outputs": [],
   "source": [
    "train_filenames = os.listdir(img_dir)\n",
    "x_train_filenames = np.array([os.path.join(img_dir, filename) for filename in train_filenames])\n",
    "y_train_filenames = np.array([os.path.join(label_dir, filename) for filename in train_filenames])\n",
    "test_filenames = os.listdir(test_dir)\n",
    "x_test_filenames = np.array([os.path.join(test_dir, filename) for filename in test_filenames])\n",
    "\n",
    "print(\"Number of training examples: {}\".format(len(x_train_filenames)))\n",
    "print(\"Number of test examples: {}\".format(len(x_test_filenames)))\n",
    "\n",
    "data_splits = []\n",
    "\n",
    "if cv_splits > 1:\n",
    "  kFold = KFold(n_splits=cv_splits, shuffle=True)\n",
    "  data_splits = [(train, val) for train, val in kFold.split(x_train_filenames)]\n",
    "else:\n",
    "  train, val = train_test_split(range(len(x_train_filenames)), test_size=0.2)\n",
    "  data_splits = [(np.array(train), np.array(val))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nhda5fkPS3JD"
   },
   "source": [
    "### Debug Output: Image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Di1N83ArilzR"
   },
   "outputs": [],
   "source": [
    "if DEBUG_OUTPUT:\n",
    "  print(\"Training images:\\n\", x_train_filenames[:5])\n",
    "  print(\"Training labels:\\n\", y_train_filenames[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mhvDoZkbcUa1"
   },
   "source": [
    "### Debug Output: x_train and y_train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qUA6SDLhozjj",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if DEBUG_OUTPUT:\n",
    "  display_num = 2\n",
    "\n",
    "  r_choices = np.random.choice(len(x_train_filenames), display_num)\n",
    "  plt.figure(figsize=(10, 10))\n",
    "\n",
    "  for i in range(0, display_num * 2, 2):\n",
    "    img_num = r_choices[i // 2]\n",
    "    x_pathname = x_train_filenames[img_num]\n",
    "    y_pathname = y_train_filenames[img_num]\n",
    "\n",
    "    plt.subplot(display_num, 2, i + 1)\n",
    "    plt.imshow(mpimg.imread(x_pathname), cmap=\"gray\")  \n",
    "    plt.subplot(display_num, 2, i + 2)\n",
    "    plt.imshow(mpimg.imread(y_pathname), cmap=\"gray\")\n",
    "\n",
    "  plt.suptitle(\"Examples of Images and their Segmentations\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zwtgius5CRKc"
   },
   "source": [
    "## Set up train and validation datasets\n",
    "Note that we apply image augmentation to our training dataset but not our validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iu5WmYmOwKrV"
   },
   "outputs": [],
   "source": [
    "tr_cfg = {\n",
    "  'is_training': True,\n",
    "  # Extract patches of given size from image. Default: None\n",
    "  'extract_patch_size': [img_shape[0], img_shape[1]],\n",
    "  # Resize input images (and label images). Resize operation is applied after patch extraction. Default: None\n",
    "  'resize': None,\n",
    "  # Adjust value range by a scale factor. Default: 1.0\n",
    "  'scale': 1 / 255.,\n",
    "  # Adjust the hue of an RGB image by random factor choosen in range [-delta, delta]. Default: None\n",
    "  'hue_delta': None,\n",
    "  # Random contrast_factor choosen in range [1/upper_bound, upper_bound]. Default: None\n",
    "  'contrast_factor_upper_bound': None,\n",
    "  # Random saturation_factor choosen in range [1/upper_bound, upper_bound]. Default: None\n",
    "  'saturation_factor_upper_bound': None,\n",
    "  # Apply random left/right flip. Default: False\n",
    "  'flip': True,\n",
    "  # Apply rotation angle (in radians) choosen in range [-rotate, rotate]. Default: None\n",
    "  'rotate': np.radians(45.0),\n",
    "  # Apply random number of 90 degree rotations. Default: False\n",
    "  'rotate90': True,\n",
    "  \n",
    "}\n",
    "tr_preprocessing_fn = functools.partial(_augment, **tr_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtzLkDFMpF0T"
   },
   "outputs": [],
   "source": [
    "val_cfg = {\n",
    "  'is_training': False,\n",
    "  # Extract patches of the given size, covering the whole image.\n",
    "  'extract_patch_size': [img_shape[0], img_shape[1]],\n",
    "  'resize': None,\n",
    "  'scale': 1 / 255.,\n",
    "  # Also predict image on flipped versions. Increases effective batch_size by a factor of 2. Default: False\n",
    "  'flip': False,\n",
    "  # Also predict image on rotated versions. Increases effective batch_size by a factor of 4. Default: False\n",
    "  'rotate90': False,\n",
    "}\n",
    "val_preprocessing_fn = functools.partial(_augment, **val_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cfg = {\n",
    "  'is_training': False,\n",
    "  # Extract patches of the given size, covering the whole image. Default: None\n",
    "  # Increases the effective batch_size by a factor of num_patches needed to cover the image.\n",
    "  'extract_patch_size': [img_shape[0], img_shape[1]],\n",
    "  'resize': None,\n",
    "  'scale': 1 / 255.,\n",
    "  # Also predict image on flipped versions. Increases effective batch_size by a factor of 2. Default: False\n",
    "  'flip': True,\n",
    "  # Also predict image on rotated versions. Increases effective batch_size by a factor of 4. Default: False\n",
    "  'rotate90': True,\n",
    "}\n",
    "test_preprocessing_fn = functools.partial(_augment, **test_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yasuvr5IbFlM"
   },
   "source": [
    "### Debug Output: Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hjoUqbPdHQej",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if DEBUG_OUTPUT:\n",
    "  display_num = 4\n",
    "  temp_ds = get_baseline_dataset(x_train_filenames, \n",
    "                                 y_train_filenames,\n",
    "                                 preproc_fn=tr_preprocessing_fn,\n",
    "                                 batch_size=1,\n",
    "                                 shuffle=False)\n",
    "  # Let's examine some of these augmented images\n",
    "  data_aug_iter = temp_ds.make_one_shot_iterator()\n",
    "  next_element = data_aug_iter.get_next()\n",
    "  \n",
    "  created_figure = False\n",
    "  \n",
    "  with tf.Session() as sess: \n",
    "    for i in range(display_num):\n",
    "      batch_of_imgs, label = sess.run(next_element)\n",
    "      \n",
    "      if not created_figure:\n",
    "        created_figure = True\n",
    "        batch_size = len(batch_of_imgs)\n",
    "        plt.figure(figsize=(16, 8*display_num*batch_size))\n",
    "      \n",
    "      for j in range(batch_size):\n",
    "        plt.subplot(2 * display_num * batch_size, 4, 2 * batch_size * i + 2 * j + 1)\n",
    "        plt.imshow(batch_of_imgs[j])\n",
    "        plt.subplot(2 * display_num * batch_size, 4, 2 * batch_size * i + 2 * j + 2)\n",
    "        plt.imshow(label[j, :, :, 0], cmap='gray')  \n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _postprocess_get_patch_offsets(img_size, patch_size):\n",
    "  # returns the top-left corner positions of every patch in the image\n",
    "  img_height = img_size[0]\n",
    "  img_width = img_size[1]\n",
    "  xs = np.linspace(0.0,\n",
    "                   img_height - patch_size[0], \n",
    "                   int(np.ceil(img_width / patch_size[0])))\n",
    "  \n",
    "  ys = np.linspace(0.0, \n",
    "                   img_width - patch_size[1], \n",
    "                   int(np.ceil(img_width / patch_size[1])))\n",
    "  xx, yy = np.meshgrid(xs, ys)\n",
    "  xy = np.stack([xx.reshape([-1]), yy.reshape([-1])], axis=1)\n",
    "  return xy\n",
    "\n",
    "def _postprocess_stitch_patches(sample_batch, img_size, patch_size, patch_offsets):\n",
    "  assert sample_batch.shape[0] == patch_offsets.shape[0], \"batch dimension should be {}, but shape is {}\".format(patch_offsets.shape[0], str(sample_batch.shape))\n",
    "  \n",
    "  combined = np.zeros(tuple(img_size))\n",
    "  one_patch = np.ones(tuple(patch_size))\n",
    "  mask = np.zeros(tuple(img_size))\n",
    "  patch_height = patch_size[0]\n",
    "  patch_width = patch_size[1]\n",
    "  \n",
    "  for i, patch in enumerate(sample_batch):\n",
    "    offset_x, offset_y = patch_offsets[i]\n",
    "    start_x = int(offset_x)\n",
    "    start_y = int(offset_y)\n",
    "    end_x = int(offset_x + patch_height)\n",
    "    end_y = int(offset_y + patch_width)\n",
    "    tmp = combined[start_x:end_x, start_y:end_y]\n",
    "    combined[start_x:end_x, start_y:end_y] += patch[:, :, 0]\n",
    "    mask[start_x:end_x, start_y:end_y] += one_patch\n",
    "  \n",
    "  combined = np.divide(combined, mask) \n",
    "  return combined\n",
    "\n",
    "def _postprocess_combine_flips(sample_batch):\n",
    "  \"\"\"Combine original predictions with flipped predictions by averaging them\"\"\"\n",
    "  # splitted[0] = original, splitted[1] = flipped\n",
    "  splitted = np.split(sample_batch, 2)\n",
    "  flip_reversed = splitted[1]\n",
    "  flip_reversed = flip_reversed[:, :, ::-1, :]\n",
    "  combined = np.mean(np.stack([splitted[0], flip_reversed], axis=0), axis=0)    \n",
    "  return combined\n",
    "\n",
    "def _postprocess_combine_rot90(sample_batch):\n",
    "  \"\"\"Combine original prediction with rotated predictions by averaging them\"\"\"\n",
    "  # splitted[0] = 0°, splitted[1] = 90°, splitted[2] = 180°, splitted[3] = 270°\n",
    "  splitted = np.split(sample_batch, 4)\n",
    "  rot_reversed = [np.rot90(splitted[i], 4-i, axes=(1,2)) for i in range(4)]\n",
    "  combined = np.mean(np.stack(rot_reversed, axis=0), axis=0)\n",
    "  return combined\n",
    "\n",
    "def _postprocess(batch, img_size, cfg):\n",
    "  if 'extract_patch_size' in cfg:\n",
    "    patch_size = cfg['extract_patch_size']\n",
    "  else:\n",
    "    patch_size = img_size\n",
    "    \n",
    "  processed = []\n",
    "  # precalculate patch offsets\n",
    "  patch_offsets = _postprocess_get_patch_offsets(img_size, patch_size)\n",
    "    \n",
    "  if 'rotate90' in cfg and cfg['rotate90']:\n",
    "    # combine flipped predictions with originals\n",
    "    batch = _postprocess_combine_rot90(batch)\n",
    "  if 'flip' in cfg and cfg['flip']:\n",
    "    # combine flipped predictions with originals\n",
    "    batch = _postprocess_combine_flips(batch)\n",
    "  \n",
    "  num_samples = batch.shape[0] / patch_offsets.shape[0]\n",
    "  print(batch.shape, patch_offsets.shape)\n",
    "  processed = [_postprocess_stitch_patches(sample, img_size, patch_size, patch_offsets) \n",
    "                                     for sample in np.split(batch, num_samples)]\n",
    "  processed = np.stack(processed, axis=0)\n",
    "  return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define losses\n",
    "Dice loss is a metric that measures overlap. More info on optimizing for Dice coefficient (our dice loss) can be found in the [paper](http://campar.in.tum.de/pub/milletari2016Vnet/milletari2016Vnet.pdf), where it was introduced. \n",
    "\n",
    "We use dice loss here because it performs better at class imbalanced problems by design. In addition, maximizing the dice coefficient and IoU metrics are the actual objectives and goals of our segmentation task. Using cross entropy is more of a proxy which is easier to maximize. Instead, we maximize our objective directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_8_hbHECUAW"
   },
   "outputs": [],
   "source": [
    "def dice_coeff(y_true, y_pred):\n",
    "  smooth = 1.0\n",
    "  intersection = tf.reduce_sum(y_true * y_pred)\n",
    "  score = (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
    "  return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "  loss = 1 - dice_coeff(y_true, y_pred)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqClGNFJdANU"
   },
   "source": [
    "Here, we'll use a specialized loss function that combines binary cross entropy and our dice loss. This is based on [individuals who competed within this competition obtaining better results empirically](https://www.kaggle.com/c/carvana-image-masking-challenge/discussion/40199). Try out your own custom losses to measure performance (e.g. bce + log(dice_loss), only bce, etc.)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_crossentropy(y_true, y_pred):\n",
    "  return losses.binary_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "  loss = tf.sqrt(losses.mean_squared_error(y_true, y_pred))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation taken from https://lars76.github.io/neural-networks/object-detection/losses-for-segmentation/\n",
    "# \n",
    "def tversky_loss(beta=0.5):\n",
    "  def loss(y_true, y_pred):\n",
    "    numerator = tf.reduce_sum(y_true * y_pred)\n",
    "    false_positives = (1.0 - y_true) * y_pred\n",
    "    false_negatives = y_true * (1.0 - y_pred)\n",
    "    denominator = y_true * y_pred + beta * false_positives + (1 - beta) * false_negatives\n",
    "\n",
    "    return numerator / (tf.reduce_sum(denominator) + tf.keras.backend.epsilon())\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "udrfi9JGB-bL"
   },
   "outputs": [],
   "source": [
    "def loss_fn(y_true, y_pred):\n",
    "  # Flatten\n",
    "  y_true = tf.reshape(y_true, [-1])\n",
    "  y_pred = tf.reshape(y_pred, [-1])\n",
    "  \n",
    "  #loss = binary_crossentropy(y_true, y_pred)\n",
    "  loss = dice_loss(y_true, y_pred)\n",
    "  #loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "  #loss = 1/24.0 * binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred) + root_mean_squared_error(y_true, y_pred) + tversky_loss(0.2)(y_true, y_pred)\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zfew1i1F6bK-"
   },
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, num_filters):\n",
    "  layer = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
    "  layer = layers.BatchNormalization()(layer)\n",
    "  layer = layers.Activation('relu')(layer)\n",
    "  layer = layers.Conv2D(num_filters, (3, 3), padding='same')(layer)\n",
    "  layer = layers.BatchNormalization()(layer)\n",
    "  layer = layers.Activation('relu')(layer)\n",
    "  layer = layers.Dropout(0.1)(layer)\n",
    "  return layer\n",
    "\n",
    "def residual_block(input_tensor, num_filters, dilation_rate=1):\n",
    "  # Shape of input_tensor: [batch_size, height, width, num_channels]\n",
    "  _, _, _, num_channels = input_tensor.shape\n",
    "  layer = input_tensor\n",
    "  layer = layers.Conv2D(num_filters, (3, 3), padding='same', dilation_rate=1)(layer)\n",
    "  layer = layers.Lambda(tf.contrib.layers.layer_norm)(layer)\n",
    "  layer = layers.Activation('relu')(layer)\n",
    "  layer = layers.Conv2D(num_filters, (3, 3), padding='same', dilation_rate=2)(layer)\n",
    "  layer = layers.Lambda(tf.contrib.layers.layer_norm)(layer)\n",
    "  \n",
    "  # Apply projection if num_filters are not equal\n",
    "  if num_filters != num_channels:\n",
    "    input_tensor = layers.Conv2D(num_filters, (1, 1), padding='same')(input_tensor)  \n",
    "  layer = layers.Add()([input_tensor, layer])\n",
    "  \n",
    "  layer = layers.Activation('relu')(layer)\n",
    "  layer = layers.Dropout(0.1)(layer)\n",
    "  return layer\n",
    "\n",
    "def residual_bottleneck_block(input_tensor, num_filters, dilation_rate=1):\n",
    "  # Shape of input_tensor: [batch_size, height, width, num_channels]\n",
    "  _, _, _, num_channels = input_tensor.shape\n",
    "  layer = layers.Conv2D(num_filters, (1, 1), padding='same')(input_tensor)\n",
    "  layer = layers.Lambda(tf.contrib.layers.layer_norm)(layer)\n",
    "  layer = layers.Activation('relu')(layer)\n",
    "  layer = layers.Conv2D(num_filters, (3, 3), padding='same', dilation_rate=dilation_rate)(layer)\n",
    "  layer = layers.Lambda(tf.contrib.layers.layer_norm)(layer)\n",
    "  \n",
    "  # Apply final projection if num_filters are not equal\n",
    "  if num_filters != num_channels:\n",
    "    layer = layers.Conv2D(num_channels, (1, 1), padding='same')(layer)  \n",
    "  layer = layers.Add()([input_tensor, layer])\n",
    "  \n",
    "  layer = layers.Activation('relu')(layer)\n",
    "  layer = layers.Dropout(0.1)(layer)\n",
    "  return layer\n",
    "\n",
    "def dilated_spatial_pyramid_pooling(input_tensor, num_filters, num_filters_out=256, dilation_rates=[1]):\n",
    "  to_concat = []\n",
    "  to_concat.append(layers.Conv2D(num_filters, (1, 1), padding='same')(input_tensor))\n",
    "  for d in dilation_rates:\n",
    "    to_concat.append(layers.Conv2D(num_filters, (3, 3), padding='same', dilation_rate=d)(input_tensor))  \n",
    "  output = layers.Concatenate(axis=-1)(to_concat)\n",
    "  output = layers.Conv2D(num_filters_out, (1, 1), padding='same')(output)\n",
    "  output = layers.Dropout(0.25)(output)\n",
    "  return output\n",
    "\n",
    "def encoder_block(input_tensor, num_filters, num_blocks=1):\n",
    "  layer = input_tensor\n",
    "  for _ in range(num_blocks):\n",
    "    layer = conv_block(layer, num_filters)\n",
    "  pooled = layers.MaxPooling2D((2, 2), strides=(2, 2))(layer)\n",
    "  #pooled = dilated_spatial_pyramid_pooling(pooled, num_filters, dilation_rates=[1,2,5])\n",
    "  return pooled, layer\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters, num_blocks=1):\n",
    "  layer = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
    "  layer = layers.Concatenate(axis=-1)([concat_tensor, layer])\n",
    "  layer = layers.BatchNormalization()(layer)\n",
    "  layer = layers.Activation('relu')(layer)\n",
    "  for _ in range(num_blocks):\n",
    "    layer = conv_block(layer, num_filters)\n",
    "  return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRLp21S_hpTn"
   },
   "outputs": [],
   "source": [
    "def createInputsOutputs():\n",
    "  inputs = layers.Input(shape=(None, None, 3))\n",
    "  #inputs = layers.Conv2D(256, (3, 3), padding='same')(inputs)\n",
    "  # 256 / 600 / 608 / 640\n",
    "  encoder0_pool, encoder0 = encoder_block(inputs, 16, 1)\n",
    "  # 128 / 300 / 304 / 320\n",
    "  encoder1_pool, encoder1 = encoder_block(encoder0_pool, 32, 1)\n",
    "  #  64 / 150 / 152 / 160\n",
    "  encoder2_pool, encoder2 = encoder_block(encoder1_pool, 64, 1)\n",
    "  #  32 /  75 /  76 /  80\n",
    "  encoder3_pool, encoder3 = encoder_block(encoder2_pool, 128, 1)\n",
    "  #  16 /  __ /  38 /  40\n",
    "  encoder4_pool, encoder4 = encoder_block(encoder3_pool, 256, 1)\n",
    "  \n",
    "  encoder5_pool, encoder5 = encoder_block(encoder4_pool, 256, 1)\n",
    "\n",
    "  decoder4 = decoder_block(encoder5, encoder4, 256, 1)\n",
    "  \n",
    "  decoder3 = decoder_block(decoder4, encoder3, 128, 1)\n",
    "  # 80\n",
    "  decoder2 = decoder_block(decoder3, encoder2, 64, 1)\n",
    "  # 160\n",
    "  decoder1 = decoder_block(decoder2, encoder1, 32, 1)\n",
    "  # 320\n",
    "  decoder0 = decoder_block(decoder1, encoder0, 16, 1)\n",
    "  # 640\n",
    "  outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)\n",
    "  return inputs, outputs\n",
    "\n",
    "def createCompiledModel():\n",
    "  inputs, outputs = createInputsOutputs()\n",
    "  model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(1e-4, clipnorm=2), \n",
    "                loss=loss_fn, \n",
    "                metrics=[binary_crossentropy, root_mean_squared_error, dice_loss])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model path\n",
    "model_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "model_path = 'runs/' + str(model_time) + '/'\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "print('Set model dir to: ' + model_path)\n",
    "\n",
    "# Copy *.py and *.ipynb files to model_path (save source code)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "for file in glob.glob(os.path.join(cwd, '*.py')):\n",
    "  shutil.copy2(file, model_path)\n",
    "for file in glob.glob(os.path.join(cwd, '*.ipynb')):\n",
    "  shutil.copy2(file, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1nHnj6199elZ"
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=4, allow_soft_placement=True))\n",
    "tf.keras.backend.set_session(sess)\n",
    "\n",
    "model = createCompiledModel()\n",
    "model.save(model_path + 'model.hdf5')\n",
    "model.summary()\n",
    "\n",
    "tf.keras.backend.get_session().close()\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJP_EvuTb4hH"
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMZcOrq5aaj1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histories = []\n",
    "cv = 0\n",
    "\n",
    "for trainIndices, valIndices in data_splits:\n",
    "  cv = cv + 1\n",
    "  if cv_splits > 1:\n",
    "    print(\"Cross validation round {} of {}\".format(cv, cv_splits))\n",
    "  save_model_path = model_path + 'weights_cv' + str(cv) + '_epoch{epoch:03d}_rmse{val_root_mean_squared_error:.4f}.hdf5'\n",
    "  \n",
    "  train_ds = get_baseline_dataset(x_train_filenames[trainIndices], \n",
    "                                  y_train_filenames[trainIndices],\n",
    "                                  preproc_fn=tr_preprocessing_fn,\n",
    "                                  batch_size=batch_size)\n",
    "  val_ds = get_baseline_dataset(x_train_filenames[valIndices], \n",
    "                                y_train_filenames[valIndices],\n",
    "                                preproc_fn=val_preprocessing_fn,\n",
    "                                batch_size=2)\n",
    "  \n",
    "  sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=4, allow_soft_placement=True))\n",
    "  tf.keras.backend.set_session(sess)\n",
    "  \n",
    "  # Create model\n",
    "  model = createCompiledModel()\n",
    "  sess.run(tf.initializers.global_variables())\n",
    "  # Create callbacks\n",
    "  tb = TrainValTensorBoard(log_dir=model_path)\n",
    "  cp = tf.keras.callbacks.ModelCheckpoint(filepath=save_model_path, monitor='val_loss', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "  es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.005, patience=15, verbose=1, mode='auto')\n",
    "  # Train model\n",
    "  history = model.fit(train_ds, \n",
    "                    steps_per_epoch=int(np.ceil(len(x_train_filenames[trainIndices]) / float(batch_size))),\n",
    "                    epochs=epochs,\n",
    "                    validation_data=val_ds,\n",
    "                    validation_steps=int(np.ceil(len(x_train_filenames[valIndices]) / float(batch_size))),\n",
    "                    callbacks=[tb, cp, es])\n",
    "  histories.append(history)\n",
    "  \n",
    "  del model\n",
    "  tf.keras.backend.get_session().close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvntxymYn8rM",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = len(histories)\n",
    "metrics = ['loss', 'root_mean_squared_error', 'dice_loss', 'binary_crossentropy']\n",
    "metric_count = len(metrics)\n",
    "\n",
    "plt.figure(figsize=(5*metric_count, 5*n))\n",
    "for i in range(n):\n",
    "  for m, metric in enumerate(metrics):\n",
    "    loss = histories[i].history[metric]\n",
    "    val_loss = histories[i].history['val_' + metric]\n",
    "    argmin_val_loss = np.argmin(val_loss)\n",
    "    epochs_range = range(len(loss))\n",
    "\n",
    "    plt.subplot(n, metric_count, (i*metric_count)+m+1)\n",
    "    plt.axvline(argmin_val_loss, color='#ff0000a0')\n",
    "    plt.plot(epochs_range, loss, label='Training', color='#ff6f42')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation', color='#4184f3')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(metric + ', model' + str(i))\n",
    "\n",
    "plt.savefig(model_path + 'loss_graph.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGFKf8yCTYbw"
   },
   "source": [
    "# Qualitative evaluation of our performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Ph7acmrCXm6"
   },
   "outputs": [],
   "source": [
    "# Load weights from latest checkpoint\n",
    "#weights_path = sorted(glob.glob(model_path + 'weights*'), reverse=True)[0]\n",
    "# Alternatively, set weight file directly\n",
    "model_path = \"runs/2019-06-26_09-20-58/\"\n",
    "weights_path = \"runs/2019-06-26_09-20-58/weights_cv1_epoch062_rmse0.0818.hdf5\"\n",
    "\n",
    "# Create model again and initialize session.\n",
    "model = createCompiledModel()\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=4, allow_soft_placement=True))\n",
    "tf.keras.backend.set_session(sess)\n",
    "sess.run(tf.initializers.global_variables())\n",
    "\n",
    "model.load_weights(weights_path)\n",
    "print(\"Loaded model weights from:\", weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Ph7acmrCXm6"
   },
   "outputs": [],
   "source": [
    "test_batch_size = 8\n",
    "val_ds = get_baseline_dataset(x_train_filenames[data_splits[0][1]], \n",
    "                              y_train_filenames[data_splits[0][1]],\n",
    "                              preproc_fn=test_preprocessing_fn,\n",
    "                              batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0GnwZ7CPaamI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize some of the outputs \n",
    "num_display=1\n",
    "colored_masks=True\n",
    "\n",
    "data_aug_iter = val_ds.make_one_shot_iterator()\n",
    "next_element = data_aug_iter.get_next()\n",
    "cols=3\n",
    "rows=num_display*test_batch_size\n",
    "index=1\n",
    "\n",
    "print(\"Hint: The channel dimension of the input image is lost in postprocessing, this is just a displaying problem\")\n",
    "\n",
    "# Running next element in our graph will produce a batch of images\n",
    "plt.figure(figsize=(5*cols, 5*rows))\n",
    "for i in range(num_display):\n",
    "  batch_of_imgs, labels = tf.keras.backend.get_session().run(next_element)\n",
    "  predicted_labels = model.predict(batch_of_imgs)\n",
    "  \n",
    "  # Run postprocessing pipeline (not needed for full images and labels, just for visualization purposes)\n",
    "  full_images = _postprocess(batch_of_imgs, [400, 400], test_cfg)\n",
    "  full_labels = _postprocess(labels, [400, 400], test_cfg)\n",
    "  full_predictions = _postprocess(predicted_labels, [400, 400], test_cfg)\n",
    "  \n",
    "  for j in range(test_batch_size):\n",
    "    img = np.array(full_images[j])\n",
    "    label = np.array(full_labels[j, :, :])\n",
    "    predicted_label = np.array(full_predictions[j, :, :])\n",
    "    \n",
    "    plt.subplot(rows, cols, index)\n",
    "    index += 1\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Input image\")\n",
    "\n",
    "    if colored_masks:\n",
    "      tp = label * predicted_label\n",
    "      fp = np.abs(tp - predicted_label)\n",
    "      fn = np.abs(tp - label)\n",
    "      plt.subplot(rows, cols, index)\n",
    "      index += 1\n",
    "      label = np.stack([fn, tp, 0.0*tp], axis=2)\n",
    "      plt.imshow(label)\n",
    "      plt.title(\"Actual Mask (false negatives in red)\")\n",
    "      plt.subplot(rows, cols, index)\n",
    "      index += 1\n",
    "      predicted_label = np.stack([fp , tp, 0.0*tp], axis=2)\n",
    "      plt.imshow(predicted_label)\n",
    "      plt.title(\"Predicted Mask (false positives in red)\")\n",
    "\n",
    "    else:\n",
    "      plt.subplot(rows, cols, index)\n",
    "      index += 1\n",
    "      plt.imshow(label, cmap=\"gray\")\n",
    "      plt.title(\"Actual Mask\")\n",
    "      plt.subplot(rows, cols, index)\n",
    "      index += 1\n",
    "      plt.imshow(predicted_label, cmap=\"gray\")\n",
    "      plt.title(\"Predicted Mask\")\n",
    "  \n",
    "#plt.suptitle(\"Examples of Input Image, Label, and Prediction\")\n",
    "plt.savefig(model_path + 'validation_examples.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Output: Raw images (patches, rotated/flipped versions if enabled, no averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if DEBUG_OUTPUT:\n",
    "  disp_imgs = batch_of_imgs\n",
    "  disp_labels = labels\n",
    "  disp_preds = predicted_labels\n",
    "  #disp_imgs = full_images.reshape([-1, 400, 400, 1])\n",
    "  #disp_labels = full_labels.reshape([-1, 400, 400, 1])\n",
    "  #disp_preds = full_predictions.reshape([-1, 400, 400, 1])\n",
    "  \n",
    "  print(\"Input batch shape:\", disp_imgs.shape)\n",
    "  print(\"Label batch shape:\", disp_labels.shape)\n",
    "  print(\"Prediction batch shape:\", disp_preds.shape)\n",
    "  print(\"Config used:\", test_cfg)\n",
    "\n",
    "  cols = 3\n",
    "  rows = disp_imgs.shape[0]\n",
    "  count = cols * rows\n",
    "  plt.figure(figsize=(4*cols, 4*rows))\n",
    "  for i in range(0, rows*cols, cols):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(disp_imgs[i // cols, :, :, 0])\n",
    "\n",
    "    plt.subplot(rows, cols, i+2)\n",
    "    plt.imshow(disp_labels[i // cols, :, :, 0], cmap=\"gray\")\n",
    "\n",
    "    plt.subplot(rows, cols, i+3)\n",
    "    plt.imshow(disp_preds[i // cols, :, :, 0], cmap=\"gray\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory for predictions\n",
    "prediction_dir = weights_path.replace(\"weights\", \"predictions\", 1).replace(\".hdf5\", \"\", 1)\n",
    "#os.mkdir(prediction_dir)\n",
    "test_batch_size=4\n",
    "test_ds = get_baseline_dataset(x_test_filenames,\n",
    "                               x_test_filenames, \n",
    "                               preproc_fn=test_preprocessing_fn,\n",
    "                               batch_size=test_batch_size,\n",
    "                               shuffle=False,\n",
    "                               repeat=False)\n",
    "\n",
    "data_aug_iter = test_ds.make_one_shot_iterator()\n",
    "next_element = data_aug_iter.get_next()\n",
    "count=0\n",
    "\n",
    "sess = tf.keras.backend.get_session()\n",
    "try:\n",
    "  while True:\n",
    "    batch_of_imgs, label = sess.run(next_element)\n",
    "    predicted_labels = model.predict(batch_of_imgs)\n",
    "    predicted_labels = _postprocess(predicted_labels, [608, 608], test_cfg)\n",
    "        \n",
    "    # rescale images from [0, 1] to [0, 255]\n",
    "    predicted_labels = predicted_labels / val_cfg['scale']\n",
    "    \n",
    "    for i in range(len(predicted_labels)):\n",
    "      pred = Image.fromarray(predicted_labels[i, :, :]).convert('L')\n",
    "      imageio.imwrite(os.path.join(prediction_dir, test_filenames[count]), pred)\n",
    "      count += 1\n",
    "except tf.errors.OutOfRangeError:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some of the predictions\n",
    "display_num = 4\n",
    "col_num = 3\n",
    "\n",
    "r_choices = np.random.choice(len(test_filenames), display_num)\n",
    "plt.figure(figsize=(5*col_num, 5*display_num))\n",
    "\n",
    "for i in range(0, display_num * col_num, col_num):\n",
    "  img_num = r_choices[i // col_num]\n",
    "  x_pathname = x_test_filenames[img_num]\n",
    "  y_pathname = os.path.join(prediction_dir, test_filenames[img_num])\n",
    "  img = np.array(mpimg.imread(x_pathname))\n",
    "  label = np.array(mpimg.imread(y_pathname))\n",
    "  \n",
    "  plt.subplot(display_num, col_num, i + 1)\n",
    "  plt.imshow(img)\n",
    "  plt.title(\"Image\")\n",
    "  \n",
    "  img[:, :, 0] = label\n",
    "  plt.subplot(display_num, col_num, i + 2)\n",
    "  plt.imshow(img)\n",
    "  plt.title(\"Image masked with label\")\n",
    "  \n",
    "  plt.subplot(display_num, col_num, i + 3)\n",
    "  plt.imshow(label, cmap=\"gray\")\n",
    "  plt.title(\"Predicted label\")\n",
    "\n",
    "#plt.suptitle(\"Examples of images and their predicted segmentations\")\n",
    "plt.savefig(model_path + 'test_examples.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask to submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "\n",
    "# assign a label to a patch\n",
    "def patch_to_label(patch):\n",
    "  df = np.mean(patch)\n",
    "  if df > foreground_threshold:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def mask_to_submission_strings(image_filename):\n",
    "  \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
    "  img_number = int(re.search(r\"\\d+(?=\\.png$)\", image_filename).group(0))\n",
    "  im = mpimg.imread(image_filename)\n",
    "  patch_size = 16\n",
    "  for j in range(0, im.shape[1], patch_size):\n",
    "    for i in range(0, im.shape[0], patch_size):\n",
    "      patch = im[i:i + patch_size, j:j + patch_size]\n",
    "      label = patch_to_label(patch)\n",
    "      yield(\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
    "\n",
    "\n",
    "def masks_to_submission(submission_filename, *image_filenames):\n",
    "  \"\"\"Converts images into a submission file\"\"\"\n",
    "  with open(submission_filename, 'w') as f:\n",
    "    f.write('id,prediction\\n')\n",
    "    for fn in image_filenames[0:]:\n",
    "      f.writelines('{}\\n'.format(s) for s in mask_to_submission_strings(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_filename = weights_path.replace(\"weights\", \"submission\", 1).replace(\".hdf5\", \".csv\", 1)\n",
    "image_filenames = [os.path.join(prediction_dir, filename) for filename in os.listdir(prediction_dir)]\n",
    "masks_to_submission(submission_filename, *image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validating submission file:\", submission_filename)\n",
    "df = pd.read_csv(submission_filename)\n",
    "\n",
    "print('Shape of csv:', df.shape)\n",
    "assert df.shape == (135736, 2), \"Invalid number of rows or columns in submission file!\"\n",
    "assert df['id'].unique().size == 135736, \"Column 'id' should contain 135736 unique values!\"\n",
    "\n",
    "meanPred = df['prediction'].mean()\n",
    "print(\"Mean prediction: {:.3f}\".format(meanPred))\n",
    "assert meanPred > 0.05 and meanPred < 0.35, \"Very unlikely mean prediction!\"\n",
    "\n",
    "print(\"Submission file looks OKAY!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = [400, 400]\n",
    "patch_shape = [256, 256]\n",
    "\n",
    "xs = np.linspace(0.0, img_shape[0] - patch_shape[0], np.ceil(img_shape[0] / patch_shape[0]).astype(int))\n",
    "ys = np.linspace(0.0, img_shape[1] - patch_shape[1], np.ceil(img_shape[1] / patch_shape[1]).astype(int))\n",
    "\n",
    "xx, yy = np.meshgrid(xs, ys)\n",
    "xy = np.stack([xx.reshape(-1), yy.reshape(-1)], axis=1)\n",
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Image Segmentation",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
